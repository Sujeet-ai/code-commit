try:
    import sys
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from pyspark.sql.functions import udf
    import hashlib
    from pyspark.sql.functions import concat_ws, udf, concat
    from awsglue.dynamicframe import DynamicFrame

except Exception as e:
    pass

sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)


## Read data from S3
s3_path = "s3://awsgluetestsim/2024/03/08/09/ok-1-2024-03-08-09-41-22-3b43b4f5-f193-47da-b21e-8a899301b014"
data_source = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={"paths": [s3_path]},
    format="json"  
)

## Apply transformations
transformed_data = data_source.resolveChoice(
    specs=[('lastErrorCode', 'cast:int')])\
    .drop_fields(['rawData'])  

## Write transformed data back to S3 in CSV format
output_path = "s3://destiantion-02/test3"
glueContext.write_dynamic_frame.from_options(
    frame=transformed_data,
    connection_type="s3",
    connection_options={"path": output_path},
    format="csv")

job.commit()
spark.stop()
